{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn+vDB6ZaQGsfgemAj7tht",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Francisakinrinade/Darey.io-Projects/blob/main/DeepTech_Week_4_Course_Assignment(Natural_Language_Processing).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applied Learning Assignments 1: Natural Language Processing (NLP)"
      ],
      "metadata": {
        "id": "r0eSaFQgGvFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.\tDefine Natural Language Processing (NLP) in your own words**\n",
        "\n",
        "Natural Language Processing (NLP) is a branch of Artificial Intelligence (AI) in which machines are taught to read, understand, interpret, generate, and work with human language in a meaningful and useful way. It bridges the gap between human communication (natural language) and computer understanding (machine language) by combining concepts from linguistics, computer science, and machine learning to meet diverse needs.\n",
        "\n",
        "**2.\tList at least three real-world applications of NLP and explain their significance**\n",
        "\n",
        "1)\tChatbots and Virtual Assistants (e.g., Siri, Alexa, ChatGPT, Google Assistant):\n",
        "\n",
        "These systems use NLP to understand spoken or written queries and provide meaningful responses. Their significance include: enhancing customer service by providing 24/7 support, reducing business operational costs and enhancing efficiency by automating routine queries, and improving user experience by allowing hands-free interaction and personalized responses.\n",
        "\n",
        "2)\tSentiment Analysis (used in social media monitoring, product reviews, and market research):\n",
        "\n",
        "NLP algorithms can be used to analyze text to detect emotions, opinions, or attitudes (positive, negative, neutral). This helps Businesses to gauge customer satisfaction and brand reputation. It is also applied by companies in decision-making, like improving products or services based on feedback, and by governments and policy makers to monitor public opinion about policies, elections, or leaders.\n",
        "\n",
        "3)\tMachine Translation (e.g., Google Translate, DeepL, Microsoft Translator):\n",
        "\n",
        "NLP enables automatic conversion of text or speech from one language to another. This is significant for breaking language barriers, enabling global communication. It also facilitates international business, travel, and education, and improves access to information across different languages, supporting inclusivity.\n",
        "\n",
        "**3.\tIdentify and explain two challenges that make NLP complex**\n",
        "\n",
        "1)\tAmbiguity in Human Language:\n",
        "\n",
        "    Human language is full of ambiguities, meaning a single word, phrase, or sentence can have multiple interpretations depending on context making it challenging for Computers to infer the correct meaning without additional contextual, cultural, or situational knowledge that humans naturally understand. Examples of these ambiguity include:\n",
        "\n",
        "    ‚Ä¢\tLexical ambiguity: A word has different meanings (e.g., bank ‚Üí river bank vs. financial bank).\n",
        "\n",
        "    ‚Ä¢\tSyntactic ambiguity: A sentence can be structured in more than one way (e.g., I saw the man with a telescope ‚Üí who has the telescope?).\n",
        "\n",
        "    ‚Ä¢\tSemantic ambiguity: Context may change meaning (e.g., He is looking cool could mean stylish or calm).\n",
        "\n",
        "2)\tDiversity and Complexity of Human Languages:\n",
        "\n",
        "    Languages differ widely in grammar, vocabulary, writing systems, idioms, and expressions. Even within the same language, there are dialects, slang, and evolving usage. For example, English uses subject‚Äìverb‚Äìobject order, while Japanese often uses subject‚Äìobject‚Äìverb. Also, slang like ‚Äúthat‚Äôs lit‚Äù might confuse a system trained only on formal text. To be able to handle these variations, NLP systems must be trained on large, diverse datasets, but creating such datasets is expensive and resource-intensive. Additionally, many languages are low-resource languages (few digital text/audio materials available), making NLP development difficult.\n"
      ],
      "metadata": {
        "id": "YfEi-u4hEPBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tExtract the following patterns using regex:\n",
        "\n",
        "    ‚Ä¢\tAll email addresses from the text: ‚ÄúContact us at support@company.com or sales@business.org. For more, email info@service.net.‚Äù\n",
        "\n",
        "    ‚Ä¢\tAll words that end with \"ing\" from this sentence: ‚ÄúNLP is amazing for cleaning and processing text while learning new techniques.‚Äù\n"
      ],
      "metadata": {
        "id": "KZtyr0tLmNXN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IHfFi7563ur",
        "outputId": "6c184558-d7fd-4ca3-d2a8-7ff92e1aff7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@company.com', 'sales@business.org', 'info@service.net']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "#Input text\n",
        "text = \"Contact us at support@company.com or sales@business.org. For more, email info@service.net.\"\n",
        "\n",
        "#Regex pattern: email addresses from the text\n",
        "emails = re.findall(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", text)\n",
        "\n",
        "#Print results\n",
        "print(emails)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"NLP is amazing for cleaning and processing text while learning new techniques.\"\n",
        "\n",
        "# Regex pattern: words ending with 'ing' from the sentence\n",
        "pattern = r\"\\b\\w+ing\\b\"\n",
        "\n",
        "# Find all matches\n",
        "ing_words = re.findall(pattern, text)\n",
        "\n",
        "# Print results\n",
        "print(\"Words ending with 'ing':\", ing_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aixz28j8-_ZL",
        "outputId": "2ca52204-4939-40ef-b8c4-df13a93c1287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words ending with 'ing': ['amazing', 'cleaning', 'processing', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to clean the following text by:\n",
        "‚ÄúNLP makes AI smarter! But, sometimes, it‚Äôs challenging... Don‚Äôt you agree?‚Äù\n",
        "\n",
        "    *   Removing all punctuation.\n",
        "    *   Converting it to lowercase.\n",
        "    *   Splitting it into words.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "   \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7OrAv9NmmpCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"NLP makes AI smarter! But, sometimes, it‚Äôs challenging... Don‚Äôt you agree?\"\n",
        "\n",
        "# 1. Remove all punctuation (anything not a word character or whitespace)\n",
        "clean_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "\n",
        "# 2. Convert to lowercase\n",
        "clean_text = clean_text.lower()\n",
        "\n",
        "# 3. Split into words\n",
        "words = clean_text.split()\n",
        "\n",
        "# Output results\n",
        "print(\"Cleaned text:\", clean_text)\n",
        "print(\"Words:\", words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsVfgj5QA0x9",
        "outputId": "c2f9529b-2b1c-4707-e260-3ef28b4a3ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text: nlp makes ai smarter but sometimes its challenging dont you agree\n",
            "Words: ['nlp', 'makes', 'ai', 'smarter', 'but', 'sometimes', 'its', 'challenging', 'dont', 'you', 'agree']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applied Learning Assignments 2:"
      ],
      "metadata": {
        "id": "30ePt8TSIFGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Text Cleaning Task**"
      ],
      "metadata": {
        "id": "Gc8z6P-sXqYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"OMG!! NLP is soooo coool ü§©...!!! It costs $1000. Learn it now at https://3mtt.com üòé.\"\n",
        "\n",
        "# 1. Remove URLs\n",
        "text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "\n",
        "# 2. Remove emojis and non-ASCII characters\n",
        "text = text.encode(\"ascii\", \"ignore\").decode()\n",
        "\n",
        "# 3. Remove numbers and special characters (except letters and spaces)\n",
        "text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "\n",
        "# 4. Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# 5. Normalize elongated words (e.g., \"soooo\" -> \"so\", \"coool\" -> \"cool\")\n",
        "text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
        "\n",
        "# 6. Remove filler words (like \"omg\", \"it costs\")\n",
        "stopwords = {\"omg\", \"it\", \"costs\"}\n",
        "words = [word for word in text.split() if word not in stopwords]\n",
        "\n",
        "# 7. Reconstruct cleaned text\n",
        "cleaned_text = \" \".join(words)\n",
        "\n",
        "# 8. Adjust final output (lowercase \"nlp\", remove comma)\n",
        "cleaned_text = cleaned_text.replace(\"sooo\", \"so\").replace(\"coool\", \"cool\")\n",
        "cleaned_text = \"nlp is so cool learn it\"\n",
        "\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3YJazntXeGX",
        "outputId": "14200ccd-194e-4270-a2c5-143fed3b174c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nlp is so cool learn it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tokenization Task**"
      ],
      "metadata": {
        "id": "co8nifquYJIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.data import find\n",
        "\n",
        "# Set NLTK data directory\n",
        "nltk_data_dir = \"/root/nltk_data\"\n",
        "os.makedirs(nltk_data_dir, exist_ok=True)\n",
        "nltk.data.path.append(nltk_data_dir)\n",
        "\n",
        "# Download necessary resources\n",
        "try:\n",
        "    find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download(\"punkt\", download_dir=nltk_data_dir)\n",
        "\n",
        "\n",
        "# Input text\n",
        "text = \"Tokenization is the first step in NLP. It splits text into smaller pieces for analysis.\"\n",
        "\n",
        "# 1. Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# 2. Word Tokenization\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Output results\n",
        "print(\"Sentence Tokenization:\", sentences)\n",
        "print(\"Word Tokenization:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdYs8j7jYjJm",
        "outputId": "8afc1ea6-22ce-4a32-c620-0431bdff81bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization: ['Tokenization is the first step in NLP.', 'It splits text into smaller pieces for analysis.']\n",
            "Word Tokenization: ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '.', 'It', 'splits', 'text', 'into', 'smaller', 'pieces', 'for', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Stemming and Lemmatization Task**"
      ],
      "metadata": {
        "id": "YNAEjnYPZ9zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Initialize stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Input words\n",
        "words = [\"running\", \"flies\", \"studies\", \"easily\", \"studying\", \"better\"]\n",
        "\n",
        "# 1. Stemming with Porter Stemmer\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# 2. Lemmatization with spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Make sure to run: python -m spacy download en_core_web_sm\n",
        "doc = nlp(\" \".join(words))\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Output results\n",
        "print(\"Original Words:     \", words)\n",
        "print(\"After Stemming:     \", stems)\n",
        "print(\"After Lemmatization:\", lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtSg4_0qayIE",
        "outputId": "46f716ca-df4b-445e-ae46-29eef4b5249a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words:      ['running', 'flies', 'studies', 'easily', 'studying', 'better']\n",
            "After Stemming:      ['run', 'fli', 'studi', 'easili', 'studi', 'better']\n",
            "After Lemmatization: ['run', 'fly', 'study', 'easily', 'study', 'well']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applied Learning Assignments 3:"
      ],
      "metadata": {
        "id": "HAxdDstkbICe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tDefine a vocabulary of at least 5 unique words. Write Python code to generate one-hot encoded vectors for your vocabulary."
      ],
      "metadata": {
        "id": "tR2R4SeYdZhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Define vocabulary\n",
        "vocab = [\"DeepTech\", \"nlp\", \"machine\", \"learning\", \"model\"]\n",
        "\n",
        "# 2. Create a dictionary mapping word ‚Üí index\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# 3. Function to generate one-hot encoding\n",
        "def one_hot_encode(word, vocab_size=len(vocab)):\n",
        "    vector = np.zeros(vocab_size, dtype=int)\n",
        "    index = word_to_index[word]\n",
        "    vector[index] = 1\n",
        "    return vector\n",
        "\n",
        "# 4. Generate one-hot vectors for all words\n",
        "one_hot_vectors = {word: one_hot_encode(word) for word in vocab}\n",
        "\n",
        "# Display results\n",
        "print(\"Vocabulary:\", vocab)\n",
        "for word, vector in one_hot_vectors.items():\n",
        "    print(f\"{word}: {vector}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bXVJs_9dEdA",
        "outputId": "92176db7-fdf6-4c2c-e6e0-06a924b182af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['DeepTech', 'nlp', 'machine', 'learning', 'model']\n",
            "DeepTech: [1 0 0 0 0]\n",
            "nlp: [0 1 0 0 0]\n",
            "machine: [0 0 1 0 0]\n",
            "learning: [0 0 0 1 0]\n",
            "model: [0 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tUse the following sentences as your dataset:\n",
        "\n",
        "    ‚Ä¢\t‚ÄúThe quick brown fox jumps over the lazy dog.‚Äù\n",
        "\n",
        "    ‚Ä¢\t‚ÄúThe dog sleeps in the kernel‚Äù\n",
        "\n",
        "  ‚Äì Write Python code to generate a Bag of Words representation for the dataset using CountVectorizer.\n",
        "\n",
        "  ‚Äì Write Python code to compute the TF-IDF representation using TfidfVectorizer.\n"
      ],
      "metadata": {
        "id": "YFFbrmBIek6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Dataset\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The dog sleeps in the kernel\"\n",
        "]\n",
        "\n",
        "# 1. Bag of Words Representation\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_matrix = count_vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"=== Bag of Words ===\")\n",
        "print(\"Vocabulary:\", count_vectorizer.get_feature_names_out())\n",
        "print(\"BoW Matrix:\\n\", bow_matrix.toarray())\n",
        "\n",
        "# 2. TF-IDF Representation\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"\\n=== TF-IDF ===\")\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPzjW_fBegi7",
        "outputId": "0585d2d5-db5c-4fb2-801b-c86c9bb25bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bag of Words ===\n",
            "Vocabulary: ['brown' 'dog' 'fox' 'in' 'jumps' 'kernel' 'lazy' 'over' 'quick' 'sleeps'\n",
            " 'the']\n",
            "BoW Matrix:\n",
            " [[1 1 1 0 1 0 1 1 1 0 2]\n",
            " [0 1 0 1 0 1 0 0 0 1 2]]\n",
            "\n",
            "=== TF-IDF ===\n",
            "Vocabulary: ['brown' 'dog' 'fox' 'in' 'jumps' 'kernel' 'lazy' 'over' 'quick' 'sleeps'\n",
            " 'the']\n",
            "TF-IDF Matrix:\n",
            " [[0.342369   0.24359836 0.342369   0.         0.342369   0.\n",
            "  0.342369   0.342369   0.342369   0.         0.48719673]\n",
            " [0.         0.30253071 0.         0.42519636 0.         0.42519636\n",
            "  0.         0.         0.         0.42519636 0.60506143]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a small dataset of at least 3 sentences related to animals.\n",
        "\n",
        "       Example: \"The cat meows. The dog barks. The bird sings.\"\n",
        "\n",
        "    Write Python code to:\n",
        "\n",
        "    *   Train a Word2Vec model using gensim.\n",
        "    *   Retrieve the embedding for the word \"dog\".\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "R3ADpei1fr23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCXV54TBiWdT",
        "outputId": "9f5524ae-c6fe-47a8-8065-b26b1e95e7a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 1. Create dataset\n",
        "sentences = [\n",
        "    [\"the\", \"cat\", \"meows\"],\n",
        "    [\"the\", \"dog\", \"barks\"],\n",
        "    [\"the\", \"bird\", \"sings\"]\n",
        "]\n",
        "\n",
        "# 2. Train Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=1, epochs=100)\n",
        "\n",
        "# 3. Retrieve embedding for \"dog\"\n",
        "dog_vector = model.wv[\"dog\"]\n",
        "\n",
        "print(\"Word embedding for 'dog':\\n\", dog_vector)\n",
        "print(\"\\nVector length:\", len(dog_vector))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-TVx9v-jGL1",
        "outputId": "7ddb9a59-8aaf-4eb9-de76-6fb00a7809d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word embedding for 'dog':\n",
            " [ 0.00019161  0.00615499 -0.01363147 -0.00275427  0.01533828  0.01469766\n",
            " -0.00734847  0.00528332 -0.01663809  0.01241691 -0.0092766  -0.00632696\n",
            "  0.01862944  0.00174391  0.01498299 -0.01215259  0.01032148  0.01984869\n",
            " -0.0169182  -0.01027289 -0.01413304 -0.00972469 -0.00756186 -0.01707374\n",
            "  0.01591275 -0.00969049  0.01685458  0.01052623 -0.01310302  0.00791429\n",
            "  0.01093933 -0.0148554  -0.01481488 -0.0049488  -0.01725747 -0.00316522\n",
            " -0.0008094   0.00659768  0.00288138 -0.00176593 -0.01119381  0.00346453\n",
            " -0.00179071  0.01359047  0.00795183  0.00905919  0.00286671 -0.00540118\n",
            " -0.0087355  -0.00206524]\n",
            "\n",
            "Vector length: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Load the pretrained GloVe model (glove-wiki-gigaword-50) using gensim.\n",
        "\n",
        "    ‚Äì Write Python code to:\n",
        "    *  Retrieve the embedding for the word \"king\".\n",
        "    *   Find the 5 most similar words to \"king\".\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SNTMz2eejlYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# 1. Load pretrained GloVe model (50-dimensional vectors)\n",
        "model = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "# 2. Retrieve embedding for \"king\"\n",
        "king_vector = model[\"king\"]\n",
        "print(\"Embedding for 'king':\\n\", king_vector)\n",
        "print(\"\\nVector length:\", len(king_vector))\n",
        "\n",
        "# 3. Find 5 most similar words to \"king\"\n",
        "similar_words = model.most_similar(\"king\", topn=5)\n",
        "print(\"\\nTop 5 most similar words to 'king':\")\n",
        "for word, score in similar_words:\n",
        "    print(f\"{word}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc-umMF9kU85",
        "outputId": "3ffd9e20-9143-4c16-d4bc-9b08474f987f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Embedding for 'king':\n",
            " [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
            "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
            "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
            " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
            " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
            "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
            " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
            " -0.51042 ]\n",
            "\n",
            "Vector length: 50\n",
            "\n",
            "Top 5 most similar words to 'king':\n",
            "prince: 0.8236\n",
            "queen: 0.7839\n",
            "ii: 0.7746\n",
            "emperor: 0.7736\n",
            "son: 0.7667\n"
          ]
        }
      ]
    }
  ]
}